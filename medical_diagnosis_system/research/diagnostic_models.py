"""
è¯Šæ–­æ¨¡å‹æ¨¡å— - å®ç°å„ç§æœºå™¨å­¦ä¹ è¯Šæ–­æ¨¡å‹
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix
import xgboost as xgb
import lightgbm as lgb
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

class DiagnosticPredictor:
    """è¯Šæ–­é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, model_type='xgboost'):
        self.model_type = model_type
        self.model = None
        self.feature_names = None
        self.training_history = {}
        
    def prepare_model(self, model_type=None):
        """å‡†å¤‡æ¨¡å‹"""
        if model_type:
            self.model_type = model_type
            
        if self.model_type == 'xgboost':
            self.model = xgb.XGBClassifier(
                objective='binary:logistic',
                eval_metric='auc',
                random_state=42,
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1
            )
        elif self.model_type == 'lightgbm':
            self.model = lgb.LGBMClassifier(
                objective='binary',
                metric='auc',
                random_state=42,
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                verbose=-1
            )
        elif self.model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
        elif self.model_type == 'logistic':
            self.model = LogisticRegression(
                random_state=42,
                max_iter=1000
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        print(f"âœ… {self.model_type}æ¨¡å‹å·²å‡†å¤‡")
        return self.model
    
    def train(self, X, y, test_size=0.2, validation=True):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ{self.model_type}æ¨¡å‹...")
        
        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
        
        self.feature_names = X.columns.tolist() if hasattr(X, 'columns') else None
        
        # è®­ç»ƒæ¨¡å‹
        if self.model_type in ['xgboost', 'lightgbm']:
            # æ”¯æŒéªŒè¯é›†çš„æ¨¡å‹
            eval_set = [(X_test, y_test)] if validation else None
            self.model.fit(
                X_train, y_train,
                eval_set=eval_set,
                verbose=False
            )
        else:
            self.model.fit(X_train, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]
        y_pred = self.model.predict(X_test)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        auc_score = roc_auc_score(y_test, y_pred_proba)
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.training_history = {
            'model_type': self.model_type,
            'train_size': len(X_train),
            'test_size': len(X_test),
            'auc_score': auc_score,
            'training_time': datetime.now().isoformat(),
            'feature_count': X.shape[1]
        }
        
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        print(f"   è®­ç»ƒé›†å¤§å°: {len(X_train)}")
        print(f"   æµ‹è¯•é›†å¤§å°: {len(X_test)}")
        print(f"   AUCå¾—åˆ†: {auc_score:.4f}")
        
        # ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š
        self._generate_evaluation_report(y_test, y_pred, y_pred_proba)
        
        return {
            'model': self.model,
            'auc_score': auc_score,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }
    
    def _generate_evaluation_report(self, y_true, y_pred, y_pred_proba):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        print("\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°:")
        
        # åˆ†ç±»æŠ¥å‘Š
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_true, y_pred))
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        print(f"\næ··æ·†çŸ©é˜µ:")
        print(f"   çœŸé˜´æ€§: {cm[0,0]}, å‡é˜³æ€§: {cm[0,1]}")
        print(f"   å‡é˜´æ€§: {cm[1,0]}, çœŸé˜³æ€§: {cm[1,1]}")
        
        # ROCæ›²çº¿
        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
        
        plt.figure(figsize=(12, 4))
        
        # ROCæ›²çº¿
        plt.subplot(1, 3, 1)
        plt.plot(fpr, tpr, label=f'ROCæ›²çº¿ (AUC = {roc_auc_score(y_true, y_pred_proba):.3f})')
        plt.plot([0, 1], [0, 1], 'k--', label='éšæœºåˆ†ç±»å™¨')
        plt.xlabel('å‡é˜³æ€§ç‡')
        plt.ylabel('çœŸé˜³æ€§ç‡')
        plt.title('ROCæ›²çº¿')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        plt.subplot(1, 3, 2)
        plt.hist(y_pred_proba[y_true == 0], bins=20, alpha=0.7, label='é˜´æ€§æ ·æœ¬', density=True)
        plt.hist(y_pred_proba[y_true == 1], bins=20, alpha=0.7, label='é˜³æ€§æ ·æœ¬', density=True)
        plt.xlabel('é¢„æµ‹æ¦‚ç‡')
        plt.ylabel('å¯†åº¦')
        plt.title('é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ')
        plt.legend()
        
        # æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
        plt.subplot(1, 3, 3)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        
        plt.tight_layout()
        plt.show()
    
    def predict(self, X):
        """é¢„æµ‹æ–°æ ·æœ¬"""
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")
        
        if hasattr(X, 'columns') and self.feature_names:
            # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
            X = X[self.feature_names]
        
        probabilities = self.model.predict_proba(X)[:, 1]
        predictions = self.model.predict(X)
        
        return predictions, probabilities
    
    def get_feature_importance(self, top_n=10):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
        elif hasattr(self.model, 'coef_'):
            importances = np.abs(self.model.coef_[0])
        else:
            print("âš ï¸  è¯¥æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
            return None
        
        if self.feature_names:
            feature_importance = pd.DataFrame({
                'feature': self.feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False)
        else:
            feature_importance = pd.DataFrame({
                'feature': [f'feature_{i}' for i in range(len(importances))],
                'importance': importances
            }).sort_values('importance', ascending=False)
        
        # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(10, 6))
        top_features = feature_importance.head(top_n)
        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('é‡è¦æ€§å¾—åˆ†')
        plt.title(f'Top {top_n} ç‰¹å¾é‡è¦æ€§')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()
        
        return feature_importance
    
    def hyperparameter_tuning(self, X, y, cv=5):
        """è¶…å‚æ•°è°ƒä¼˜"""
        print(f"ğŸ”§ å¼€å§‹{self.model_type}è¶…å‚æ•°è°ƒä¼˜...")
        
        if self.model_type == 'xgboost':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 6, 9],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 0.9, 1.0]
            }
        elif self.model_type == 'lightgbm':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [3, 6, 9],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 0.9, 1.0]
            }
        elif self.model_type == 'random_forest':
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [5, 10, 15],
                'min_samples_split': [2, 5, 10]
            }
        else:
            param_grid = {
                'C': [0.1, 1, 10],
                'penalty': ['l1', 'l2']
            }
        
        grid_search = GridSearchCV(
            self.model, param_grid,
            cv=cv, scoring='roc_auc',
            n_jobs=-1, verbose=1
        )
        
        grid_search.fit(X, y)
        
        print(f"âœ… æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"âœ… æœ€ä½³CVå¾—åˆ†: {grid_search.best_score_:.4f}")
        
        self.model = grid_search.best_estimator_
        return grid_search.best_params_, grid_search.best_score_
    
    def save_model(self, file_path):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ")
        
        model_data = {
            'model': self.model,
            'model_type': self.model_type,
            'feature_names': self.feature_names,
            'training_history': self.training_history
        }
        
        joblib.dump(model_data, file_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {file_path}")
    
    def load_model(self, file_path):
        """åŠ è½½æ¨¡å‹"""
        model_data = joblib.load(file_path)
        
        self.model = model_data['model']
        self.model_type = model_data['model_type']
        self.feature_names = model_data['feature_names']
        self.training_history = model_data.get('training_history', {})
        
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {file_path}")
        print(f"   æ¨¡å‹ç±»å‹: {self.model_type}")
        print(f"   ç‰¹å¾æ•°é‡: {len(self.feature_names) if self.feature_names else 'Unknown'}")
        
        return self.model


class MultiClassDiagnosticPredictor(DiagnosticPredictor):
    """å¤šåˆ†ç±»è¯Šæ–­é¢„æµ‹å™¨"""
    
    def __init__(self, model_type='xgboost', n_classes=3):
        super().__init__(model_type)
        self.n_classes = n_classes
        
    def prepare_model(self, model_type=None):
        """å‡†å¤‡å¤šåˆ†ç±»æ¨¡å‹"""
        if model_type:
            self.model_type = model_type
            
        if self.model_type == 'xgboost':
            self.model = xgb.XGBClassifier(
                objective='multi:softprob',
                eval_metric='mlogloss',
                random_state=42,
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1
            )
        elif self.model_type == 'lightgbm':
            self.model = lgb.LGBMClassifier(
                objective='multiclass',
                metric='multi_logloss',
                random_state=42,
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                verbose=-1
            )
        elif self.model_type == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
        elif self.model_type == 'logistic':
            self.model = LogisticRegression(
                random_state=42,
                max_iter=1000,
                multi_class='ovr'
            )
        
        print(f"âœ… {self.model_type}å¤šåˆ†ç±»æ¨¡å‹å·²å‡†å¤‡")
        return self.model


def create_diagnostic_pipeline(data_path, target_column, model_type='xgboost'):
    """åˆ›å»ºå®Œæ•´çš„è¯Šæ–­æ¨¡å‹è®­ç»ƒæµæ°´çº¿"""
    print("ğŸ”¬ å¯åŠ¨è¯Šæ–­æ¨¡å‹è®­ç»ƒæµæ°´çº¿...")
    
    # 1. åŠ è½½æ•°æ®
    from .data_engineering import DataProcessor
    processor = DataProcessor()
    
    df = processor.load_data(data_path)
    if df is None:
        return None
    
    # 2. æ•°æ®æ¸…æ´—å’Œç‰¹å¾å·¥ç¨‹
    df_clean = processor.clean_data(df)
    df_features = processor.feature_engineering(df_clean)
    
    # 3. å‡†å¤‡MLæ•°æ®
    X, y = processor.prepare_ml_data(df_features, target_column)
    
    # 4. è®­ç»ƒæ¨¡å‹
    predictor = DiagnosticPredictor(model_type)
    predictor.prepare_model()
    
    results = predictor.train(X, y)
    
    # 5. ç‰¹å¾é‡è¦æ€§åˆ†æ
    feature_importance = predictor.get_feature_importance()
    
    # 6. ä¿å­˜æ¨¡å‹
    model_path = f"models/diagnostic_{model_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
    predictor.save_model(model_path)
    
    return {
        'predictor': predictor,
        'results': results,
        'feature_importance': feature_importance,
        'model_path': model_path
    }
