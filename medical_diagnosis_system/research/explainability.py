"""
æ¨¡å‹å¯è§£é‡Šæ€§åˆ†ææ¨¡å— - SHAPåˆ†æã€ç‰¹å¾é‡è¦æ€§ã€ä¸ç¡®å®šåº¦ä¼°è®¡
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Optional

class ModelExplainer:
    """æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æå™¨"""
    
    def __init__(self):
        self.shap_explainer = None
        self.feature_names = None
        
    def setup_shap_explainer(self, model, X_background, model_type='tree'):
        """è®¾ç½®SHAPè§£é‡Šå™¨"""
        try:
            import shap
            
            if model_type == 'tree':
                # ç”¨äºXGBoostã€LightGBMã€RandomForest
                self.shap_explainer = shap.TreeExplainer(model)
            elif model_type == 'linear':
                # ç”¨äºçº¿æ€§æ¨¡å‹
                self.shap_explainer = shap.LinearExplainer(model, X_background)
            elif model_type == 'kernel':
                # é€šç”¨è§£é‡Šå™¨ï¼Œè¾ƒæ…¢ä½†æ”¯æŒæ‰€æœ‰æ¨¡å‹
                self.shap_explainer = shap.KernelExplainer(model.predict_proba, X_background)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
            
            self.feature_names = X_background.columns.tolist() if hasattr(X_background, 'columns') else None
            print(f"âœ… SHAP {model_type}è§£é‡Šå™¨å·²è®¾ç½®")
            
        except ImportError:
            print("âŒ éœ€è¦å®‰è£…SHAP: pip install shap")
            return None
        except Exception as e:
            print(f"âŒ SHAPè§£é‡Šå™¨è®¾ç½®å¤±è´¥: {e}")
            return None
    
    def explain_prediction(self, X_sample, plot_explanation=True):
        """è§£é‡Šå•ä¸ªé¢„æµ‹"""
        if self.shap_explainer is None:
            print("âŒ SHAPè§£é‡Šå™¨å°šæœªè®¾ç½®")
            return None
        
        try:
            import shap
            
            # è®¡ç®—SHAPå€¼
            shap_values = self.shap_explainer.shap_values(X_sample)
            
            # å¦‚æœæ˜¯äºŒåˆ†ç±»ï¼Œå–æ­£ç±»çš„SHAPå€¼
            if isinstance(shap_values, list):
                shap_values = shap_values[1]
            
            # ç¡®ä¿æ˜¯2Dæ•°ç»„
            if len(shap_values.shape) == 1:
                shap_values = shap_values.reshape(1, -1)
            
            # è·å–ç‰¹å¾å€¼
            if hasattr(X_sample, 'values'):
                feature_values = X_sample.values[0] if len(X_sample.shape) > 1 else X_sample.values
            else:
                feature_values = X_sample[0] if len(X_sample.shape) > 1 else X_sample
            
            # åˆ›å»ºç‰¹å¾è´¡çŒ®åˆ†æ
            feature_contributions = {}
            if self.feature_names:
                for i, feature_name in enumerate(self.feature_names):
                    if i < len(shap_values[0]):
                        feature_contributions[feature_name] = {
                            "shap_value": float(shap_values[0][i]),
                            "feature_value": float(feature_values[i]),
                            "contribution_type": "æ­£å‘" if shap_values[0][i] > 0 else "è´Ÿå‘"
                        }
            
            # æ’åºç‰¹å¾è´¡çŒ®
            sorted_contributions = sorted(
                feature_contributions.items(),
                key=lambda x: abs(x[1]["shap_value"]),
                reverse=True
            )
            
            # æå–topå› ç´ 
            top_positive = []
            top_negative = []
            
            for feature, contrib in sorted_contributions[:10]:
                if contrib["shap_value"] > 0:
                    top_positive.append({
                        "feature": feature,
                        "contribution": contrib["shap_value"],
                        "value": contrib["feature_value"]
                    })
                else:
                    top_negative.append({
                        "feature": feature,
                        "contribution": abs(contrib["shap_value"]),
                        "value": contrib["feature_value"]
                    })
            
            # ç”Ÿæˆè§£é‡Šæ€§æ€»ç»“
            explanation_summary = self._generate_explanation_summary(
                top_positive[:3], top_negative[:3]
            )
            
            # ç»˜åˆ¶è§£é‡Šå›¾
            if plot_explanation:
                self._plot_shap_explanation(shap_values, X_sample, feature_contributions)
            
            return {
                "shap_values": shap_values.tolist(),
                "feature_contributions": feature_contributions,
                "top_positive": top_positive[:5],
                "top_negative": top_negative[:5],
                "summary": explanation_summary
            }
            
        except Exception as e:
            print(f"âŒ SHAPè§£é‡Šå¤±è´¥: {e}")
            return None
    
    def _generate_explanation_summary(self, top_positive: List, top_negative: List) -> str:
        """ç”Ÿæˆè§£é‡Šæ€§æ€»ç»“"""
        summary_parts = []
        
        if top_positive:
            positive_features = [f"{item['feature']}({item['value']:.2f})" for item in top_positive]
            summary_parts.append(f"ä¸»è¦é£é™©å› ç´ : {', '.join(positive_features)}")
        
        if top_negative:
            negative_features = [f"{item['feature']}({item['value']:.2f})" for item in top_negative]
            summary_parts.append(f"ä¸»è¦ä¿æŠ¤å› ç´ : {', '.join(negative_features)}")
        
        return "; ".join(summary_parts)
    
    def _plot_shap_explanation(self, shap_values, X_sample, feature_contributions):
        """ç»˜åˆ¶SHAPè§£é‡Šå›¾"""
        try:
            import shap
            
            # åˆ›å»ºå›¾å½¢
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            
            # ç€‘å¸ƒå›¾
            if hasattr(shap, 'waterfall_plot'):
                shap.waterfall_plot(
                    shap.Explanation(
                        values=shap_values[0],
                        base_values=self.shap_explainer.expected_value,
                        data=X_sample.values[0] if hasattr(X_sample, 'values') else X_sample,
                        feature_names=self.feature_names
                    ),
                    show=False
                )
                plt.subplot(1, 2, 1)
                plt.title("SHAPç€‘å¸ƒå›¾")
            
            # ç‰¹å¾è´¡çŒ®æ¡å½¢å›¾
            plt.subplot(1, 2, 2)
            
            # é€‰æ‹©top10ç‰¹å¾
            sorted_features = sorted(
                feature_contributions.items(),
                key=lambda x: abs(x[1]["shap_value"]),
                reverse=True
            )[:10]
            
            features = [item[0] for item in sorted_features]
            values = [item[1]["shap_value"] for item in sorted_features]
            colors = ['red' if v > 0 else 'blue' for v in values]
            
            plt.barh(range(len(features)), values, color=colors, alpha=0.7)
            plt.yticks(range(len(features)), features)
            plt.xlabel('SHAPå€¼ (å¯¹é¢„æµ‹çš„è´¡çŒ®)')
            plt.title('ç‰¹å¾è´¡çŒ®åº¦åˆ†æ')
            plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
            
            plt.tight_layout()
            plt.show()
            
        except Exception as e:
            print(f"âš ï¸  SHAPå¯è§†åŒ–å¤±è´¥: {e}")
    
    def global_feature_importance(self, X_data, model, top_n=20):
        """å…¨å±€ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        if self.shap_explainer is None:
            print("âŒ SHAPè§£é‡Šå™¨å°šæœªè®¾ç½®")
            return None
        
        try:
            import shap
            
            # è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„SHAPå€¼
            shap_values = self.shap_explainer.shap_values(X_data)
            
            if isinstance(shap_values, list):
                shap_values = shap_values[1]  # å–æ­£ç±»
            
            # è®¡ç®—å…¨å±€é‡è¦æ€§
            global_importance = np.abs(shap_values).mean(0)
            
            # åˆ›å»ºé‡è¦æ€§DataFrame
            if self.feature_names:
                importance_df = pd.DataFrame({
                    'feature': self.feature_names,
                    'importance': global_importance
                }).sort_values('importance', ascending=False)
            else:
                importance_df = pd.DataFrame({
                    'feature': [f'feature_{i}' for i in range(len(global_importance))],
                    'importance': global_importance
                }).sort_values('importance', ascending=False)
            
            # ç»˜åˆ¶å…¨å±€é‡è¦æ€§
            plt.figure(figsize=(12, 8))
            top_features = importance_df.head(top_n)
            
            plt.subplot(2, 1, 1)
            plt.barh(range(len(top_features)), top_features['importance'])
            plt.yticks(range(len(top_features)), top_features['feature'])
            plt.xlabel('å¹³å‡|SHAPå€¼|')
            plt.title(f'å…¨å±€ç‰¹å¾é‡è¦æ€§ (Top {top_n})')
            plt.gca().invert_yaxis()
            
            # SHAP summary plot
            plt.subplot(2, 1, 2)
            if hasattr(shap, 'summary_plot'):
                shap.summary_plot(shap_values, X_data, feature_names=self.feature_names, show=False)
                plt.title('SHAPç‰¹å¾å½±å“åˆ†å¸ƒ')
            
            plt.tight_layout()
            plt.show()
            
            return importance_df
            
        except Exception as e:
            print(f"âŒ å…¨å±€é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
            return None
    
    def analyze_feature_interactions(self, X_sample, feature_pairs=None):
        """åˆ†æç‰¹å¾äº¤äº’æ•ˆåº”"""
        if self.shap_explainer is None:
            print("âŒ SHAPè§£é‡Šå™¨å°šæœªè®¾ç½®")
            return None
        
        try:
            import shap
            
            if feature_pairs is None and self.feature_names:
                # è‡ªåŠ¨é€‰æ‹©é‡è¦ç‰¹å¾å¯¹
                importance = self.global_feature_importance(X_sample, None, top_n=5)
                if importance is not None:
                    top_features = importance.head(5)['feature'].tolist()
                    feature_pairs = [(top_features[i], top_features[j]) 
                                   for i in range(len(top_features)) 
                                   for j in range(i+1, len(top_features))][:3]
            
            if feature_pairs and hasattr(shap, 'dependence_plot'):
                fig, axes = plt.subplots(1, len(feature_pairs), figsize=(5*len(feature_pairs), 4))
                if len(feature_pairs) == 1:
                    axes = [axes]
                
                shap_values = self.shap_explainer.shap_values(X_sample)
                if isinstance(shap_values, list):
                    shap_values = shap_values[1]
                
                for i, (feat1, feat2) in enumerate(feature_pairs):
                    if feat1 in self.feature_names and feat2 in self.feature_names:
                        feat1_idx = self.feature_names.index(feat1)
                        
                        plt.subplot(1, len(feature_pairs), i+1)
                        shap.dependence_plot(
                            feat1_idx, shap_values, X_sample,
                            interaction_index=feat2,
                            feature_names=self.feature_names,
                            show=False
                        )
                        plt.title(f'{feat1} vs {feat2}äº¤äº’æ•ˆåº”')
                
                plt.tight_layout()
                plt.show()
            
            return feature_pairs
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾äº¤äº’åˆ†æå¤±è´¥: {e}")
            return None


class UncertaintyEstimator:
    """ä¸ç¡®å®šåº¦ä¼°è®¡å™¨"""
    
    @staticmethod
    def bootstrap_confidence_interval(model, X, y, n_bootstrap=100, confidence_level=0.95):
        """Bootstrapç½®ä¿¡åŒºé—´ä¼°è®¡"""
        print(f"ğŸ”„ è¿›è¡ŒBootstrapä¸ç¡®å®šåº¦ä¼°è®¡ (n={n_bootstrap})...")
        
        bootstrap_scores = []
        n_samples = len(X)
        
        for i in range(n_bootstrap):
            # Bootstrapé‡‡æ ·
            indices = np.random.choice(n_samples, n_samples, replace=True)
            X_boot = X.iloc[indices] if hasattr(X, 'iloc') else X[indices]
            y_boot = y.iloc[indices] if hasattr(y, 'iloc') else y[indices]
            
            # è®­ç»ƒæ¨¡å‹
            model_copy = model.__class__(**model.get_params())
            model_copy.fit(X_boot, y_boot)
            
            # è¯„ä¼°æ€§èƒ½
            if hasattr(model_copy, 'predict_proba'):
                y_pred_proba = model_copy.predict_proba(X)[:, 1]
                from sklearn.metrics import roc_auc_score
                score = roc_auc_score(y, y_pred_proba)
            else:
                from sklearn.metrics import accuracy_score
                y_pred = model_copy.predict(X)
                score = accuracy_score(y, y_pred)
            
            bootstrap_scores.append(score)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        alpha = 1 - confidence_level
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        ci_lower = np.percentile(bootstrap_scores, lower_percentile)
        ci_upper = np.percentile(bootstrap_scores, upper_percentile)
        mean_score = np.mean(bootstrap_scores)
        std_score = np.std(bootstrap_scores)
        
        print(f"âœ… Bootstrapç»“æœ:")
        print(f"   å¹³å‡æ€§èƒ½: {mean_score:.4f} Â± {std_score:.4f}")
        print(f"   {confidence_level*100}%ç½®ä¿¡åŒºé—´: [{ci_lower:.4f}, {ci_upper:.4f}]")
        
        return {
            'mean_score': mean_score,
            'std_score': std_score,
            'confidence_interval': (ci_lower, ci_upper),
            'bootstrap_scores': bootstrap_scores
        }
    
    @staticmethod
    def prediction_uncertainty(model, X_sample, n_iterations=100):
        """é¢„æµ‹ä¸ç¡®å®šåº¦ä¼°è®¡"""
        if not hasattr(model, 'predict_proba'):
            print("âš ï¸  æ¨¡å‹ä¸æ”¯æŒæ¦‚ç‡é¢„æµ‹ï¼Œæ— æ³•ä¼°è®¡ä¸ç¡®å®šåº¦")
            return None
        
        # ä½¿ç”¨æ¨¡å‹çš„å†…åœ¨ä¸ç¡®å®šæ€§ï¼ˆå¦‚æœæ”¯æŒï¼‰
        predictions = []
        
        for _ in range(n_iterations):
            # æ·»åŠ å°é‡å™ªå£°æ¥ä¼°è®¡ä¸ç¡®å®šæ€§
            noise_scale = 0.01
            X_noisy = X_sample + np.random.normal(0, noise_scale, X_sample.shape)
            pred_proba = model.predict_proba(X_noisy)[:, 1]
            predictions.append(pred_proba[0])
        
        mean_pred = np.mean(predictions)
        std_pred = np.std(predictions)
        
        # è®¡ç®—ç½®ä¿¡åŒºé—´
        ci_lower = np.percentile(predictions, 2.5)
        ci_upper = np.percentile(predictions, 97.5)
        
        uncertainty_level = "ä½" if std_pred < 0.05 else "ä¸­" if std_pred < 0.1 else "é«˜"
        
        return {
            'mean_prediction': mean_pred,
            'std_prediction': std_pred,
            'confidence_interval': (ci_lower, ci_upper),
            'uncertainty_level': uncertainty_level,
            'all_predictions': predictions
        }
    
    @staticmethod
    def calibration_analysis(y_true, y_pred_proba, n_bins=10):
        """æ¨¡å‹æ ¡å‡†åˆ†æ"""
        from sklearn.calibration import calibration_curve
        
        # è®¡ç®—æ ¡å‡†æ›²çº¿
        fraction_of_positives, mean_predicted_value = calibration_curve(
            y_true, y_pred_proba, n_bins=n_bins
        )
        
        # ç»˜åˆ¶æ ¡å‡†æ›²çº¿
        plt.figure(figsize=(10, 6))
        
        plt.subplot(1, 2, 1)
        plt.plot(mean_predicted_value, fraction_of_positives, "s-", label="æ¨¡å‹æ ¡å‡†")
        plt.plot([0, 1], [0, 1], "k:", label="å®Œç¾æ ¡å‡†")
        plt.xlabel("å¹³å‡é¢„æµ‹æ¦‚ç‡")
        plt.ylabel("å®é™…é˜³æ€§æ¯”ä¾‹")
        plt.title("æ ¡å‡†æ›²çº¿")
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # é¢„æµ‹æ¦‚ç‡ç›´æ–¹å›¾
        plt.subplot(1, 2, 2)
        plt.hist(y_pred_proba, bins=20, alpha=0.7, density=True)
        plt.xlabel("é¢„æµ‹æ¦‚ç‡")
        plt.ylabel("å¯†åº¦")
        plt.title("é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ")
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # è®¡ç®—Brier Score
        brier_score = np.mean((y_pred_proba - y_true) ** 2)
        
        return {
            'calibration_curve': (fraction_of_positives, mean_predicted_value),
            'brier_score': brier_score,
            'is_well_calibrated': brier_score < 0.1
        }


class ClinicalDecisionSupport:
    """ä¸´åºŠå†³ç­–æ”¯æŒ"""
    
    @staticmethod
    def generate_risk_stratification(evidence_bundle: Dict) -> Dict:
        """ç”Ÿæˆé£é™©åˆ†å±‚å»ºè®®"""
        risk_stratification = {
            "overall_risk": "ä¸­ç­‰",
            "risk_factors": [],
            "protective_factors": [],
            "actionable_recommendations": []
        }
        
        # ç»¼åˆå„æ¨¡å‹çš„é£é™©è¯„ä¼°
        diagnostic_prob = evidence_bundle.get('diagnostic_prediction', {}).get('probability', 0)
        survival_risk = evidence_bundle.get('survival_prediction', {}).get('risk_group', '')
        recurrence_prob = evidence_bundle.get('recurrence_prediction', {}).get('recurrence_probability_2yr', 0)
        
        # è®¡ç®—ç»¼åˆé£é™©åˆ†æ•°
        risk_score = 0
        if diagnostic_prob > 0.8:
            risk_score += 2
        elif diagnostic_prob > 0.6:
            risk_score += 1
        
        if 'é«˜å±' in survival_risk:
            risk_score += 2
        elif 'ä¸­é«˜å±' in survival_risk:
            risk_score += 1
        
        if recurrence_prob > 0.5:
            risk_score += 2
        elif recurrence_prob > 0.3:
            risk_score += 1
        
        # ç¡®å®šæ€»ä½“é£é™©ç­‰çº§
        if risk_score >= 5:
            risk_stratification["overall_risk"] = "é«˜å±"
        elif risk_score >= 3:
            risk_stratification["overall_risk"] = "ä¸­é«˜å±"
        elif risk_score >= 1:
            risk_stratification["overall_risk"] = "ä¸­ç­‰"
        else:
            risk_stratification["overall_risk"] = "ä½å±"
        
        # ç”Ÿæˆå…·ä½“å»ºè®®
        if risk_stratification["overall_risk"] in ["é«˜å±", "ä¸­é«˜å±"]:
            risk_stratification["actionable_recommendations"].extend([
                "å»ºè®®å¤šå­¦ç§‘å›¢é˜Ÿä¼šè¯Š",
                "è€ƒè™‘ç§¯æçš„ç»¼åˆæ²»ç–—æ–¹æ¡ˆ",
                "å¯†åˆ‡ç›‘æµ‹å’Œéšè®¿"
            ])
        
        return risk_stratification
    
    @staticmethod
    def generate_treatment_timeline(evidence_bundle: Dict) -> Dict:
        """ç”Ÿæˆæ²»ç–—æ—¶é—´çº¿å»ºè®®"""
        timeline = {
            "immediate": [],      # ç«‹å³ï¼ˆ1-3å¤©ï¼‰
            "short_term": [],     # çŸ­æœŸï¼ˆ1-2å‘¨ï¼‰
            "medium_term": [],    # ä¸­æœŸï¼ˆ1-3æœˆï¼‰
            "long_term": []       # é•¿æœŸï¼ˆ>3æœˆï¼‰
        }
        
        # åŸºäºé£é™©åˆ†å±‚åˆ¶å®šæ—¶é—´çº¿
        diagnostic_prob = evidence_bundle.get('diagnostic_prediction', {}).get('probability', 0)
        
        if diagnostic_prob > 0.8:
            timeline["immediate"].append("å®Œå–„å½±åƒå­¦æ£€æŸ¥")
            timeline["short_term"].append("ä¸“ç§‘ä¼šè¯Šå’Œæ²»ç–—æ–¹æ¡ˆåˆ¶å®š")
            timeline["medium_term"].append("å®æ–½æ²»ç–—æ–¹æ¡ˆ")
            timeline["long_term"].append("é•¿æœŸéšè®¿å’Œç›‘æµ‹")
        
        return timeline
